# ğŸ¦ **Lauki Finance: Credit Risk Modelling (Classification Model)**

*A portfolio project built as part of the CodeBasics Gen AI & Data Science Bootcamp*

---

## ğŸš€ **Project Overview**

This project is a complete **end-to-end Credit Risk Modelling solution** that predicts the **default probability** of loan applicants and generates a **credit score (300â€“900)** along with actionable risk ratings (Excellent â†’ Poor).

The project simulates a real engagement with **Lauki Finance**, an NBFC that provides loans to customers underserved by traditional banks. Their rapid growth demands a modern, AI-powered system to evaluate borrower risk efficiently and consistently.

The final solution includes:

- A full data cleaning & preprocessing pipeline
- Exploratory data analysis & business-rule validation
- Advanced feature engineering
- Model training with imbalance handling (RUS + SMOTETomek)
- Hyperparameter tuning using Optuna
- Model packaging & versioning
- Deployment-ready Streamlit UI
- Automated PDF credit report generator

---

## ğŸ“Œ **Live App:**

ğŸ”— **[https://ml-project-credit-risk-modelling-codebasics.streamlit.app/](https://ml-project-credit-risk-modelling-codebasics.streamlit.app/)**

## ğŸ“Œ **GitHub Repository:**

ğŸ”— [https://github.com/ruchitha-meenakshi/ml-project-credit-risk-modelling](https://github.com/ruchitha-meenakshi/ml-project-credit-risk-modelling)

---

# ğŸ§© **Business Story: Lauki Financeâ€™s Transformation**

After the success of the S.H.I.E.L.D. Insurance project, Bruce Harley and his AI startup **AtliQ.ai** quickly gained industry attention.
One of the first calls came from **Steve Singh**, the Head of **Lauki Finance**, an NBFC serving borrowers who are unable to obtain loans from traditional banks.

### **The problem?**

Lauki Finance relied heavily on:

* Manual credit evaluation
* Inconsistent decisions
* Slow processing times
* High operational overhead

These bottlenecks restricted business growth and limited their ability to scale.

Seeing potential in Steveâ€™s vision, Bruce reached out to Tony at AtliQ.ai to build a system that could:

âš¡ Automate credit decisioning  
âš¡ Reduce dependency on manual reviews  
âš¡ Improve risk prediction accuracy  
âš¡ Support faster loan approvals  
âš¡ Maintain transparency & explainability for regulatory needs

Tony assigned **Peter** as the project lead. What began as a â€œsmall assignmentâ€ turned into a foundational AI initiative for Lauki Finance.

---

# ğŸ¯ **Project Objectives**

### ğŸ¯ Primary Goal

Build a machine learning model that predicts **loan default probability** and generates an **interpretable credit score**.

### ğŸ“Œ Success Criteria (Defined by Lauki Finance)

| Metric                        | Target                                       |
| ----------------------------- | -------------------------------------------- |
| **Recall (Default Class)**    | > 90%                                        |
| **Precision (Default Class)** | > 50%                                        |
| **Explainability**            | Must support rule-based interpretation       |
| **Rank Ordering**             | Must exhibit monotonic decile-level ordering |
| **Real-time Deployment**      | Streamlit web app for decision automation    |

---

# ğŸ§± **Project Structure**

```
ml-project-credit-risk-modelling
â”‚
â”œâ”€â”€ app/                              # Streamlit web application
â”‚   â”œâ”€â”€ main.py                       # UI + feature collection
â”‚   â”œâ”€â”€ prediction_helper.py          # Feature engineering + model scoring
â”‚   â””â”€â”€ report_generator.py           # PDF credit report generator
â”‚
â”œâ”€â”€ artifacts/                        # Final model, scaler & metadata
â”‚   â””â”€â”€ model_data.joblib
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # NOT uploaded (proprietary)
â”‚   â””â”€â”€ processed/                    # Cleaned datasets NOT uploaded (proprietary)
â”‚
â”œâ”€â”€ outputs/                          # EDA, model evaluation artifacts
â”‚   â”œâ”€â”€ figures/                      # ROC, KS, Rank Ordering plots
â”‚   â”œâ”€â”€ models/                       # EDA/testing pickles
â”‚   â””â”€â”€ tables/                       # Rank Ordering tables
â”‚
â”œâ”€â”€ scripts/                          # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_cleaning_eda.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â”œâ”€â”€ 04_model_evaluation.ipynb
â”‚   â””â”€â”€ imports.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# ğŸ§  **Technical Stack**

### ğŸ’» Languages & Libraries

* Python 3.10
* Pandas, NumPy
* Scikit-Learn
* XGBoost
* Imbalanced-learn (SMOTE-Tomek)
* Optuna (Hyperparameter tuning)
* Matplotlib, Seaborn
* Joblib
* Streamlit
* FPDF (PDF report generation)

---

# ğŸ” **Model Overview**

This project addresses a **binary classification** problem where the objective is to estimate the likelihood of a borrower defaulting on a loan. The target variable is defined as follows:

| Target Class | Value      | Business Interpretation                                 |
| ------------ | ---------- | ------------------------------------------------------- |
| **0**        | No Default | Customer is expected to repay (Good / Non-Event)        |
| **1**        | Default    | Customer is likely to default (Bad / Event of Interest) |

Lauki Financeâ€™s business requirement strongly emphasizes **high recall on the defaulter class**, ensuring fewer risky applicants are incorrectly approved.

### ğŸ“Œ **Models Evaluated**

A range of models were explored to balance predictive performance and explainability:

| Model                   | Rationale                                                                                                               |
| ----------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Logistic Regression** | Highly interpretable; aligns with financial regulatory expectations; easy to translate coefficients into business rules |
| **Random Forest**       | Strong non-linear modelling capability; good baseline for complex interactions                                          |
| **XGBoost**             | High predictive power; excellent handling of tabular data, though lower interpretability                                |

Logistic Regression was selected as the **final model** due to its strong performance, interpretability, and alignment with business constraints.

---

## **Data Cleaning & Business Rule Enforcement**

Prior to modelling, several domain-driven validation rules were applied to ensure data quality and regulatory readiness:

### Business Rule Validations

| Rule                                   | Description                                | Outcome                    |
| -------------------------------------- | ------------------------------------------ | -------------------------- |
| **Processing Fee â‰¤ 3% of Loan Amount** | Ensures fees are realistic                 | 5 records removed          |
| **GST â‰¤ 20%**                          | Tax amount validation                      | No violations detected     |
| **Net Disbursement â‰¤ Loan Amount**     | Prevents negative or inflated disbursement | No violations detected     |
| **Missing Values Handling**            | Categorical â†’ Mode; Numeric â†’ Domain logic | All missing values imputed |

These checks ensured a **clean and credible modelling dataset** aligned with financial industry standards.

---

## **Feature Engineering**

A combination of domain knowledge and statistical insights informed the creation of high-impact features:

| Feature                                  | Contribution                                                    |
| ---------------------------------------- | --------------------------------------------------------------- |
| **Loan-to-Income Ratio**                 | Primary indicator of repayment capacity                         |
| **Delinquency Ratio (%)**                | Measures historical repayment discipline                        |
| **Avg DPD per Delinquency**              | Captures severity of past delays                                |
| **Credit Utilization Ratio (%)**         | Reflects overall credit stress                                  |
| **One-hot encoded categorical features** | Enables behavioural segmentation                                |
| **MinMax Scaling**                       | Stabilizes model training and ensures coefficient comparability |

All transformations were persisted to ensure **consistent preprocessing during deployment**.

---

## **Class Imbalance Strategy**

The dataset exhibited a **significant imbalance** toward non-defaulters. Two approaches were tested:

### 1ï¸âƒ£ **Random Undersampling**

* Balanced the classes but removed valuable majority-class information.

### 2ï¸âƒ£ **SMOTE + Tomek Links (Final Approach)**

* Generated synthetic minority samples while removing borderline/noisy observations.
* Provided the most stable performance with strong recall on the minority (default) class.

This combination produced a balanced training dataset without distorting real-world distributions.

---

## **Hyperparameter Optimization (Optuna)**

A targeted Optuna search optimized Logistic Regression parameters, exploring:

* Regularization strength (**C**)
* Optimization algorithms (**solver**)
* Convergence threshold (**tol**)
* Class weighting strategies

**Best Parameters Identified:**

```text
C = 4.46
solver = "saga"
tol = 6.3e-06
class_weight = "balanced"
```

These settings maximized recall while maintaining model stability and interpretability.

---

## ğŸ“ˆ **Model Performance Evaluation**

### **Classification Metrics (Default Class)**

| Metric        | Value    | Status                           |
| ------------- | -------- | -------------------------------- |
| **Recall**    | **94%**  | âœ” Meets business target          |
| **Precision** | **56%**  | âœ” Acceptable (with human review) |
| **F1-Score**  | **0.70** | Balanced performance             |

---

### **Robustness Metrics**

| Metric                   | Value     | Interpretation                                |
| ------------------------ | --------- | --------------------------------------------- |
| **ROCâ€“AUC**              | **0.98**  | Exceptional discriminatory power              |
| **Gini Coefficient**     | **0.96**  | Strong rank ordering capability               |
| **KS Statistic**         | **85.9%** | Excellent separation of good vs bad customers |
| **Decile Rank Ordering** | Achieved  | Higher deciles capture higher-risk borrowers  |

Collectively, these metrics confirm that the model is **deployment-ready**, exhibits strong discriminatory power, and satisfies Lauki Financeâ€™s operational and regulatory needs.

---

# ğŸŒ **Streamlit App**

The deployed Streamlit application enables:

âœ” Real-time data entry  
âœ” Automated feature engineering  
âœ” Probability of default calculation  
âœ” Credit score generation (300â€“900)  
âœ” Risk category: Excellent / Good / Average / Poor  
âœ” Downloadable PDF credit report  
âœ” Clean, modern UI with custom CSS

ğŸ“Œ **Live App:** [https://ml-project-credit-risk-modelling-codebasics.streamlit.app/](https://ml-project-credit-risk-modelling-codebasics.streamlit.app/)

## ğŸ§¾ **Automated PDF Report Generation**

After generating a prediction, the app allows users to **download a complete credit decision report** as a PDF.

**The report includes:**

* Default Probability
* Calculated Credit Score
* Final Rating
* Full list of model input parameters

This feature simulates a **real NBFC workflow**, enabling easy auditing and documentation of credit decisions.

ğŸ“Œ **Sample PDF Output:**  
<img width="772" height="647" alt="Screenshot 2025-11-28 at 11 03 15" src="https://github.com/user-attachments/assets/4e1a0be8-f0c3-45b1-a949-83deb897376e" />

---

# ğŸ¨ **App Preview**

### ğŸ¥ Demo Video

https://github.com/user-attachments/assets/4adfe1b8-4013-4069-88e9-e27b2c472db0

---

# ğŸ”’ **Data Privacy Notice**

The dataset used for this project is provided exclusively as part of the CodeBasics Bootcamp and is **NOT publicly distributable**.

To comply with licensing:

* `data/raw/` and `data/processed/` are added to `.gitignore`
* Only `.gitkeep` placeholder files are included
* No proprietary data is uploaded

---

# ğŸ›  **How to Run Locally**

### Step 1 â€” Clone the repository

```bash
git clone https://github.com/ruchitha-meenakshi/ml-project-credit-risk-modelling.git
cd ml-project-credit-risk-modelling
```

### Step 2 â€” Install dependencies

```bash
pip install -r requirements.txt
```

### Step 3 â€” Launch the application

```bash
streamlit run app/main.py
```

---

# ğŸŒ± **Learnings from the Project**

This project helped me strengthen:

âœ” Business-first problem framing
âœ” Data cleaning using domain rules, not just statistics
âœ” Feature engineering for credit datasets
âœ” Model training on imbalanced classification problems
âœ” Hyperparameter tuning with Optuna
âœ” Understanding of regulatory requirements for ML models
âœ” Building a production-grade Streamlit app
âœ” PDF generation & real-time scoring workflow
âœ” Managing model artifacts & reproducibility

---

# ğŸ™Œ **Acknowledgements**

Special thanks to:

* **CodeBasics Bootcamp** â€“ for industry-grade project design
* **Dhaval Patel, Hemanand Vadivel & Team** â€“ for guidance
* **AtliQ.ai & Lauki Finance fictional team** â€“ for driving the narrative

---

# ğŸ‘©â€ğŸ’» **Author**

**Ruchitha Uppuluri**
Aspiring Data Scientist | CodeBasics ML Bootcamp

ğŸ”— LinkedIn: [https://www.linkedin.com/in/ruchithauppuluri](https://www.linkedin.com/in/ruchithauppuluri)

---
