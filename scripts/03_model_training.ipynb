{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10087831",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color:blue\">Model Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b7e77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c343eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and target training variables for model training\n",
    "X_train = pd.read_parquet(\"../data/processed/final_X_train.parquet\")\n",
    "\n",
    "y_train = pd.read_parquet(\"../data/processed/final_y_train.parquet\").squeeze()  # converts back to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0005084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37488, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39ce91e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(37488,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f113e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and target testing variables for model training\n",
    "X_test = pd.read_parquet(\"../data/processed/final_X_test.parquet\")\n",
    "\n",
    "y_test = pd.read_parquet(\"../data/processed/final_y_test.parquet\").squeeze()  # converts back to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea26de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12497, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c665e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(12497,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2d4bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    34265\n",
       "1     3223\n",
       "Name: default, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c6e32",
   "metadata": {},
   "source": [
    "### **Business Success Criteria**\n",
    "\n",
    "The business team defined three key model expectations:\n",
    "\n",
    "1. **Recall (on default class = 1) > 90%**  \n",
    "Missing a defaulter is extremely costly, so capturing as many default cases as possible is critical.\n",
    "\n",
    "2. **Precision > 50% for default class**  \n",
    "False positives are acceptable because flagged customers undergo human review.\n",
    "\n",
    "3. **High explainability**  \n",
    "The modelâ€™s decisions must be interpretable and convertible into business rules for the internal BRE (Business Rule Engine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d382e1f",
   "metadata": {},
   "source": [
    "## Attempt 1 - No Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d02c79",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f509c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     11423\n",
      "           1       0.84      0.72      0.78      1074\n",
      "\n",
      "    accuracy                           0.96     12497\n",
      "   macro avg       0.91      0.85      0.88     12497\n",
      "weighted avg       0.96      0.96      0.96     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "report_lr = classification_report(y_test, y_pred_lr)\n",
    "print(report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe523083",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f3a2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     11423\n",
      "           1       0.85      0.71      0.78      1074\n",
      "\n",
      "    accuracy                           0.96     12497\n",
      "   macro avg       0.91      0.85      0.88     12497\n",
      "weighted avg       0.96      0.96      0.96     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the Random Forest Classifier model\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf14236",
   "metadata": {},
   "source": [
    "### XGBoost Classifier Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb3765ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     11423\n",
      "           1       0.82      0.76      0.79      1074\n",
      "\n",
      "    accuracy                           0.97     12497\n",
      "   macro avg       0.90      0.87      0.89     12497\n",
      "weighted avg       0.96      0.97      0.96     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the XGBoost Classifier model\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "print(report_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d44c44",
   "metadata": {},
   "source": [
    "### **Baseline Model Performance**\n",
    "\n",
    "We evaluated three models without any class-imbalance handling or hyperparameter tuning:\n",
    "\n",
    "* **Logistic Regression**\n",
    "* **Random Forest**\n",
    "* **XGBoost**\n",
    "\n",
    "All three models produced similar performance:\n",
    "\n",
    "| Model               | Precision (Class 1) | Recall (Class 1) | F1 Score (Class 1) |\n",
    "| ------------------- | ------------------- | ---------------- | ------------------ |\n",
    "| Logistic Regression | 0.84                | 0.72             | 0.78               |\n",
    "| Random Forest       | 0.86                | 0.70             | 0.77               |\n",
    "| XGBoost             | 0.82                | 0.76             | 0.79               |\n",
    "\n",
    "**Key Finding:**  \n",
    "None of the baseline models achieved the required **recall â‰¥ 90%** on the default class.\n",
    "\n",
    "Given the importance of explainability, **Logistic Regression** was selected as the base model for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3401c4b",
   "metadata": {},
   "source": [
    "### RandomizedSearch CV for Attempt 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecc69a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best Parameters: {'solver': 'liblinear', 'C': 1438.44988828766}\n",
      "Best Score: 0.7578820896729831\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     11423\n",
      "           1       0.83      0.74      0.78      1074\n",
      "\n",
      "    accuracy                           0.96     12497\n",
      "   macro avg       0.90      0.86      0.88     12497\n",
      "weighted avg       0.96      0.96      0.96     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- RandomizedSearchCV Attempt 1: Logistic Regression ---\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 20),  # Logarithmically spaced values from 10^-4 to 10^4\n",
    "    'solver': ['lbfgs', 'saga', 'liblinear', 'newton-cg']   # Algorithm to use in the optimization problem\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=10000)  # Increased max_iter for convergence\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    scoring='f1',\n",
    "    cv=3,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,  # Set a random state for reproducibility\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best Score: {random_search.best_score_}\")\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029a742",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning: Logistic Regression (RandomizedSearchCV)**\n",
    "\n",
    "RandomizedSearchCV was applied to optimize:\n",
    "\n",
    "* Regularization parameter (`C`)\n",
    "* Solver selection (`lbfgs`, `liblinear`, `saga`, etc.)\n",
    "\n",
    "**Best Results:**\n",
    "\n",
    "* Best CV F1 Score: **~0.758**\n",
    "* Test Performance:\n",
    "\n",
    "  * Precision (Class 1): **0.83**\n",
    "  * Recall (Class 1): **0.74**\n",
    "\n",
    "**Conclusion:**  \n",
    "Even after tuning, Logistic Regression could not meet the business recall threshold.\n",
    "However, its explainability makes it a strong candidate for the final stage *if performance improves after class-imbalance handling*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34c4e4",
   "metadata": {},
   "source": [
    "### RandomizedSearch CV for Attempt 1: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56dccf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best Parameters: {'subsample': 0.7, 'scale_pos_weight': 2, 'reg_lambda': 10.0, 'reg_alpha': 0.01, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.2, 'colsample_bytree': 0.8}\n",
      "Best Score: 0.7885351531443989\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     11423\n",
      "           1       0.76      0.84      0.80      1074\n",
      "\n",
      "    accuracy                           0.96     12497\n",
      "   macro avg       0.87      0.91      0.89     12497\n",
      "weighted avg       0.97      0.96      0.96     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- RandomizedSearchCV Attempt 1: XGBoost ---\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'scale_pos_weight': [1, 2, 3, 5, 7, 10],\n",
    "    'reg_alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],  # L1 regularization term\n",
    "    'reg_lambda': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]  # L2 regularization term\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, n_iter=100,\n",
    "                                   scoring='f1', cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best Score: {random_search.best_score_}\")\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5ccb8",
   "metadata": {},
   "source": [
    "\n",
    "### **Hyperparameter Tuning: XGBoost (RandomizedSearchCV)**\n",
    "\n",
    "A broader parameter search was performed, tuning:\n",
    "\n",
    "* learning rate\n",
    "* depth\n",
    "* number of trees\n",
    "* subsample ratios\n",
    "* colsample ratios\n",
    "* regularization terms\n",
    "* class-imbalance weight (scale_pos_weight)\n",
    "\n",
    "**Best Results:**\n",
    "\n",
    "* Best CV F1 Score: **~0.789**\n",
    "* Test Performance:\n",
    "\n",
    "  * Precision (Class 1): **0.76**\n",
    "  * Recall (Class 1): **0.84**\n",
    "\n",
    "**Conclusion:**  \n",
    "XGBoost achieved a meaningful improvement in **recall**, but still falls short of the 90% threshold required by the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f917e8",
   "metadata": {},
   "source": [
    "All the above model trainings so far were performed **without addressing class imbalance**, even though defaults are only ~9% of the data.\n",
    "\n",
    "This is likely capping recall performance across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac94ea",
   "metadata": {},
   "source": [
    "## Attempt 2 - Handling Class Imbalance using Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90824816",
   "metadata": {},
   "source": [
    "### **Attempt 2 â€” Random UnderSampling**\n",
    "\n",
    "To boost recall on the minority class (defaults), we first applied **Random UnderSampling (RUS)**, which balances the dataset by reducing the majority class to match the minority.\n",
    "\n",
    "| Class           | Count Before | Count After RUS |\n",
    "| --------------- | ------------ | --------------- |\n",
    "| 0 (Non-default) | ~34,265      | 3,223           |\n",
    "| 1 (Default)     | ~3,223       | 3,223           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aaeccde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3223\n",
       "1    3223\n",
       "Name: default, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "y_train_rus.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90490d4",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8823f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     11423\n",
      "           1       0.51      0.96      0.67      1074\n",
      "\n",
      "    accuracy                           0.92     12497\n",
      "   macro avg       0.75      0.93      0.81     12497\n",
      "weighted avg       0.95      0.92      0.93     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression on under-sampled data\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# Evaluate on original test set\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "report_lr = classification_report(y_test, y_pred_lr)\n",
    "print(report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813e988",
   "metadata": {},
   "source": [
    "### XGBoost Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c74ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     11423\n",
      "           1       0.52      0.98      0.68      1074\n",
      "\n",
      "    accuracy                           0.92     12497\n",
      "   macro avg       0.76      0.95      0.82     12497\n",
      "weighted avg       0.96      0.92      0.93     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost using best params from previous RandomizedSearchCV\n",
    "model_xgb = XGBClassifier(**random_search.best_params_)\n",
    "model_xgb.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "print(report_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a28739",
   "metadata": {},
   "source": [
    "#### ðŸ” **Model Performance (on original test data)**\n",
    "\n",
    "Both Logistic Regression and XGBoost achieved:\n",
    "\n",
    "* **Recall â‰ˆ 96â€“98%** for the default class\n",
    "* **Precision â‰ˆ 51â€“52%**\n",
    "\n",
    "This technically meets the business requirements:\n",
    "\n",
    "* Recall > 90% âœ”ï¸\n",
    "* Precision > 50% âœ”ï¸\n",
    "* Explainability possible (LogReg) âœ”ï¸\n",
    "\n",
    "However, the overall model becomes more unstable because RUS **throws away a large portion of real data**, which risks losing important patterns from the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f07be",
   "metadata": {},
   "source": [
    "### Attempt 3 - Handling Class Imbalance using Over Sampling (SMOTE Tomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad81dc",
   "metadata": {},
   "source": [
    "To preserve majority class information, I next applied **SMOTE-Tomek**, which:\n",
    "\n",
    "* Oversamples the minority using synthetic examples (SMOTE)\n",
    "* Cleans ambiguous boundary samples (Tomek Links)\n",
    "\n",
    "| Class | Count Before | Count After SMOTE-Tomek |\n",
    "| ----- | ------------ | ----------------------- |\n",
    "| 0     | ~34,265      | 34,195                  |\n",
    "| 1     | ~3,223       | 34,195                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95073d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    34195\n",
       "1    34195\n",
       "Name: default, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt = SMOTETomek(random_state=42)\n",
    "\n",
    "X_train_smt, y_train_smt = smt.fit_resample(X_train, y_train)\n",
    "y_train_smt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc3f95",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "054b36fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     11423\n",
      "           1       0.55      0.94      0.70      1074\n",
      "\n",
      "    accuracy                           0.93     12497\n",
      "   macro avg       0.77      0.94      0.83     12497\n",
      "weighted avg       0.96      0.93      0.94     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression on SMOTE-Tomek data\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_smt, y_train_smt)\n",
    "\n",
    "# Evaluate on original test set\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "report_lr = classification_report(y_test, y_pred_lr)\n",
    "print(report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2292f84",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Using Optuna\n",
    "\n",
    "Given the strong recall achieved after handling class imbalance, we will now use **Optuna**, a state-of-the-art automatic hyperparameter optimization framework.\n",
    "\n",
    "The focus will be on **maximizing recall on the default class**, while ensuring:\n",
    "\n",
    "* Precision â‰¥ 50%\n",
    "* Coefficients remain interpretable for the BRE (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d686985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 16:06:36,431] A new study created in memory with name: no-name-6acbb002-9eed-45bc-9ecd-b0f5b19ad188\n",
      "[I 2025-11-26 16:06:36,713] Trial 0 finished with value: 0.9458100405749379 and parameters: {'C': 4.601624393735309, 'solver': 'lbfgs', 'tol': 0.05698211950822554, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:36,956] Trial 1 finished with value: 0.9457081210202624 and parameters: {'C': 8.891498340983787, 'solver': 'lbfgs', 'tol': 0.00012263839814062895, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:37,190] Trial 2 finished with value: 0.9456794085990863 and parameters: {'C': 2189.2283942354647, 'solver': 'lbfgs', 'tol': 0.0048309933188447505, 'class_weight': None}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:37,379] Trial 3 finished with value: 0.9451843781952952 and parameters: {'C': 130.31041418317076, 'solver': 'saga', 'tol': 0.026230364883651068, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:37,595] Trial 4 finished with value: 0.9371098622816524 and parameters: {'C': 0.03523908898733518, 'solver': 'lbfgs', 'tol': 7.610052284889327e-05, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:37,845] Trial 5 finished with value: 0.945665003428486 and parameters: {'C': 1376.9192069078622, 'solver': 'saga', 'tol': 0.0015080635404897912, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:38,078] Trial 6 finished with value: 0.9448500219322659 and parameters: {'C': 1635.788487789998, 'solver': 'liblinear', 'tol': 0.04073852532508003, 'class_weight': None}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:38,296] Trial 7 finished with value: 0.9456940732140916 and parameters: {'C': 90.70313710436824, 'solver': 'saga', 'tol': 0.002733883917726569, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:38,521] Trial 8 finished with value: 0.9456727194654787 and parameters: {'C': 2.4654224678208134, 'solver': 'liblinear', 'tol': 0.015256112908810621, 'class_weight': None}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:38,723] Trial 9 finished with value: 0.9150034781943686 and parameters: {'C': 0.006422488691551794, 'solver': 'liblinear', 'tol': 7.034136505130114e-05, 'class_weight': None}. Best is trial 0 with value: 0.9458100405749379.\n",
      "/Users/ruchithau/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/ruchithau/anaconda3/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "[I 2025-11-26 16:06:39,107] Trial 10 finished with value: 0.8523942521809532 and parameters: {'C': 0.000247080910721498, 'solver': 'newton-cg', 'tol': 1.6227169938761263e-05, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:39,351] Trial 11 finished with value: 0.9458090963608585 and parameters: {'C': 2.518265605559022, 'solver': 'lbfgs', 'tol': 2.868025813554618e-06, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:39,573] Trial 12 finished with value: 0.9424894501504095 and parameters: {'C': 0.15464752759931918, 'solver': 'lbfgs', 'tol': 3.7056795266687664e-06, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:39,802] Trial 13 finished with value: 0.9452644086212612 and parameters: {'C': 0.6185514830396646, 'solver': 'lbfgs', 'tol': 1.1529064265840311e-06, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:40,095] Trial 14 finished with value: 0.9456205216342938 and parameters: {'C': 15.98324534259459, 'solver': 'newton-cg', 'tol': 0.0007517789270195858, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:40,289] Trial 15 finished with value: 0.9225692040756317 and parameters: {'C': 0.00417125548872906, 'solver': 'lbfgs', 'tol': 1.3699723540911931e-05, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:40,511] Trial 16 finished with value: 0.9449558724475119 and parameters: {'C': 0.46418693297835056, 'solver': 'lbfgs', 'tol': 0.09921825804510759, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:40,737] Trial 17 finished with value: 0.9456061912978367 and parameters: {'C': 36.88410003623243, 'solver': 'lbfgs', 'tol': 0.0003280991710652686, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.9458100405749379.\n",
      "[I 2025-11-26 16:06:41,086] Trial 18 finished with value: 0.9458245878376202 and parameters: {'C': 4.083407430266515, 'solver': 'newton-cg', 'tol': 1.5701716176479944e-05, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:41,364] Trial 19 finished with value: 0.9405644375028341 and parameters: {'C': 0.08367861334199167, 'solver': 'newton-cg', 'tol': 1.9449557105893186e-05, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:41,674] Trial 20 finished with value: 0.9456648044554358 and parameters: {'C': 330.2507960853232, 'solver': 'newton-cg', 'tol': 0.00038559778713378806, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "/Users/ruchithau/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/ruchithau/anaconda3/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "[I 2025-11-26 16:06:42,048] Trial 21 finished with value: 0.945809889182562 and parameters: {'C': 3.811486355903618, 'solver': 'newton-cg', 'tol': 1.1193729918475005e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:42,523] Trial 22 finished with value: 0.9457953805024056 and parameters: {'C': 4.625108320056408, 'solver': 'newton-cg', 'tol': 4.4795150316425084e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:42,839] Trial 23 finished with value: 0.9452352819694149 and parameters: {'C': 0.6477583295095939, 'solver': 'newton-cg', 'tol': 1.2897067405647137e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:43,177] Trial 24 finished with value: 0.9456060818747941 and parameters: {'C': 24.944201225823093, 'solver': 'newton-cg', 'tol': 7.691526019952004e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:43,464] Trial 25 finished with value: 0.9456794085990863 and parameters: {'C': 7620.77058406889, 'solver': 'newton-cg', 'tol': 0.00961914087027171, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:43,774] Trial 26 finished with value: 0.9457941881417492 and parameters: {'C': 2.2557906407206545, 'solver': 'newton-cg', 'tol': 4.74925083488285e-05, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:44,073] Trial 27 finished with value: 0.931815927518664 and parameters: {'C': 0.01309082986935924, 'solver': 'newton-cg', 'tol': 2.773323488447983e-05, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:44,343] Trial 28 finished with value: 0.9456356561811426 and parameters: {'C': 306.27984290885644, 'solver': 'saga', 'tol': 0.0001332625111660688, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:44,606] Trial 29 finished with value: 0.9457369891932695 and parameters: {'C': 8.490317909411338, 'solver': 'liblinear', 'tol': 0.00019925498040604818, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:44,895] Trial 30 finished with value: 0.9438694091099568 and parameters: {'C': 0.2678918453316903, 'solver': 'newton-cg', 'tol': 8.057485219558185e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 16:06:45,134] Trial 31 finished with value: 0.9457792542852985 and parameters: {'C': 2.077673214531126, 'solver': 'lbfgs', 'tol': 2.3951323326842274e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:45,418] Trial 32 finished with value: 0.9457665622389632 and parameters: {'C': 7.339868124945498, 'solver': 'lbfgs', 'tol': 2.593729806413816e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:45,654] Trial 33 finished with value: 0.9456208359010468 and parameters: {'C': 39.20730454009977, 'solver': 'lbfgs', 'tol': 1.0014140963873619e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:45,885] Trial 34 finished with value: 0.9456614654546674 and parameters: {'C': 1.395847261497748, 'solver': 'lbfgs', 'tol': 5.617657419358513e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:46,099] Trial 35 finished with value: 0.9372417043160839 and parameters: {'C': 0.03628601231323135, 'solver': 'lbfgs', 'tol': 2.008810910088155e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:46,403] Trial 36 finished with value: 0.9456354945573398 and parameters: {'C': 100.09971122632082, 'solver': 'saga', 'tol': 8.912303607187334e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:46,631] Trial 37 finished with value: 0.9457665143658393 and parameters: {'C': 6.751947251355282, 'solver': 'lbfgs', 'tol': 4.150651317913291e-05, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:46,849] Trial 38 finished with value: 0.9399000524533645 and parameters: {'C': 0.11539915575436183, 'solver': 'liblinear', 'tol': 0.0023776626329997186, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:47,091] Trial 39 finished with value: 0.9456208759573208 and parameters: {'C': 376.6764986375323, 'solver': 'saga', 'tol': 0.0006644208743383854, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:47,389] Trial 40 finished with value: 0.9456205216342938 and parameters: {'C': 16.167200961696967, 'solver': 'newton-cg', 'tol': 0.0049077544368039475, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:47,711] Trial 41 finished with value: 0.9457953805024056 and parameters: {'C': 4.67574691850729, 'solver': 'newton-cg', 'tol': 3.477017980057388e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:48,042] Trial 42 finished with value: 0.9457805765364847 and parameters: {'C': 3.560836136105156, 'solver': 'newton-cg', 'tol': 4.429049762823327e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:48,348] Trial 43 finished with value: 0.9453964552168346 and parameters: {'C': 0.7315667475382915, 'solver': 'newton-cg', 'tol': 1.7723911836053842e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:48,663] Trial 44 finished with value: 0.9456172122027334 and parameters: {'C': 1.172633880621538, 'solver': 'newton-cg', 'tol': 4.699971590598295e-06, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:48,942] Trial 45 finished with value: 0.9456500368669327 and parameters: {'C': 51.90318300335596, 'solver': 'liblinear', 'tol': 1.446629838387028e-05, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:49,176] Trial 46 finished with value: 0.9456351324406503 and parameters: {'C': 13.827654913097534, 'solver': 'lbfgs', 'tol': 0.0321687126007673, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:49,468] Trial 47 finished with value: 0.9440161191429617 and parameters: {'C': 0.28562624118330765, 'solver': 'newton-cg', 'tol': 1.6835331415140822e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n",
      "[I 2025-11-26 16:06:49,678] Trial 48 finished with value: 0.9457803811413137 and parameters: {'C': 3.223333657995727, 'solver': 'lbfgs', 'tol': 0.09531338632069379, 'class_weight': None}. Best is trial 18 with value: 0.9458245878376202.\n",
      "/Users/ruchithau/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/ruchithau/anaconda3/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "[I 2025-11-26 16:06:50,088] Trial 49 finished with value: 0.9120960632235136 and parameters: {'C': 0.0016067276064550288, 'solver': 'newton-cg', 'tol': 3.201107555246801e-06, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.9458245878376202.\n"
     ]
    }
   ],
   "source": [
    "# --- Attempt 3: Optuna Hyperparameter Tuning on Logistic Regression (with SMOTE-Tomek data) ---\n",
    "\n",
    "# Define Optuna objective function for Logistic Regression\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 1e-4, 1e4, log=True),  # Logarithmically spaced values\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga', 'newton-cg']),  # Solvers\n",
    "        'tol': trial.suggest_float('tol', 1e-6, 1e-1, log=True),  # Logarithmically spaced values for tolerance\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])  # Class weights\n",
    "    }\n",
    "\n",
    "    model = LogisticRegression(**param, max_iter=10000)\n",
    "    \n",
    "    # Calculate the cross-validated f1_score\n",
    "    f1_scorer = make_scorer(f1_score, average='macro')\n",
    "    scores = cross_val_score(model, X_train_smt, y_train_smt, cv=3, scoring=f1_scorer, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna study\n",
    "study_logistic = optuna.create_study(direction=\"maximize\")\n",
    "study_logistic.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70589aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  F1-score: 0.9458245878376202\n",
      "  Params: \n",
      "    C: 4.083407430266515\n",
      "    solver: newton-cg\n",
      "    tol: 1.5701716176479944e-05\n",
      "    class_weight: None\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     11423\n",
      "           1       0.56      0.94      0.70      1074\n",
      "\n",
      "    accuracy                           0.93     12497\n",
      "   macro avg       0.78      0.94      0.83     12497\n",
      "weighted avg       0.96      0.93      0.94     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best trial:')\n",
    "trial = study_logistic.best_trial\n",
    "print('  F1-score: {}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "# Train best model on full SMOTE-Tomek data\n",
    "best_model_logistic = LogisticRegression(**study_logistic.best_params)\n",
    "best_model_logistic.fit(X_train_smt, y_train_smt)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model_logistic.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f2e97",
   "metadata": {},
   "source": [
    "To improve performance, we tuned **Logistic Regression** using Optuna with the SMOTE-Tomek balanced dataset.\n",
    "\n",
    "Optuna identified the best hyperparameters:\n",
    "\n",
    "* **C â‰ˆ 4.46**\n",
    "* **solver = saga**\n",
    "* **tol â‰ˆ 6.3e-06**\n",
    "* **class_weight = balanced**\n",
    "\n",
    "### **Performance (on original test set)**\n",
    "\n",
    "* **Recall (Default = 1): 94%**\n",
    "* **Precision (Default = 1): 56%**\n",
    "* **F1 Score (Default = 1): 70%**\n",
    "\n",
    "This satisfies **all business success criteria**:\n",
    "\n",
    "* Recall > 90% \n",
    "* Precision > 50% \n",
    "* Logistic Regression is fully explainable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd5b03",
   "metadata": {},
   "source": [
    "### Attempt 4 - Handling Class Imbalance using Over Sampling (SMOTE Tomek) and Hyperparameter tuning using Optuna on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5da3de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 16:06:50,638] A new study created in memory with name: no-name-61fbf846-e2e9-4586-b2f9-39701eff046c\n",
      "[I 2025-11-26 16:06:51,179] Trial 0 finished with value: 0.9572071510654218 and parameters: {'lambda': 4.155069497762944, 'alpha': 0.11839334321891563, 'subsample': 0.8908432387911362, 'colsample_bytree': 0.6616987444634201, 'max_depth': 3, 'eta': 0.2603914084680019, 'gamma': 5.767812566022825, 'scale_pos_weight': 3.598462405007485, 'min_child_weight': 9, 'max_delta_step': 5}. Best is trial 0 with value: 0.9572071510654218.\n",
      "[I 2025-11-26 16:06:51,619] Trial 1 finished with value: 0.8960593965894447 and parameters: {'lambda': 0.001546872758954236, 'alpha': 0.003063418128079918, 'subsample': 0.6137324370214805, 'colsample_bytree': 0.4639167878874908, 'max_depth': 5, 'eta': 0.0294895258190542, 'gamma': 5.095274945927662, 'scale_pos_weight': 4.2597494212776255, 'min_child_weight': 1, 'max_delta_step': 8}. Best is trial 0 with value: 0.9572071510654218.\n",
      "[I 2025-11-26 16:06:52,249] Trial 2 finished with value: 0.9722621595814029 and parameters: {'lambda': 0.012640332163571516, 'alpha': 2.259130964662967, 'subsample': 0.6842824352196725, 'colsample_bytree': 0.573579119951533, 'max_depth': 10, 'eta': 0.24502678660641664, 'gamma': 0.07267613457062061, 'scale_pos_weight': 5.674885685545528, 'min_child_weight': 10, 'max_delta_step': 5}. Best is trial 2 with value: 0.9722621595814029.\n",
      "[I 2025-11-26 16:06:52,844] Trial 3 finished with value: 0.970800700620984 and parameters: {'lambda': 2.3091937718370485, 'alpha': 0.09588776212538742, 'subsample': 0.4293529911683874, 'colsample_bytree': 0.6297376300667001, 'max_depth': 8, 'eta': 0.22140749596629675, 'gamma': 1.943244377971366, 'scale_pos_weight': 2.617447769461395, 'min_child_weight': 8, 'max_delta_step': 10}. Best is trial 2 with value: 0.9722621595814029.\n",
      "[I 2025-11-26 16:06:53,286] Trial 4 finished with value: 0.9660637048022304 and parameters: {'lambda': 0.12510422726621706, 'alpha': 0.04517908504625053, 'subsample': 0.7266942538960015, 'colsample_bytree': 0.9145515104176299, 'max_depth': 9, 'eta': 0.2890774318218869, 'gamma': 8.577002781963882, 'scale_pos_weight': 3.7289167112799837, 'min_child_weight': 7, 'max_delta_step': 5}. Best is trial 2 with value: 0.9722621595814029.\n",
      "[I 2025-11-26 16:06:53,755] Trial 5 finished with value: 0.9569273567616379 and parameters: {'lambda': 0.0018288186319219259, 'alpha': 0.22069050863532938, 'subsample': 0.5982687791044144, 'colsample_bytree': 0.4484827781442878, 'max_depth': 9, 'eta': 0.10261845411979686, 'gamma': 9.841421606294244, 'scale_pos_weight': 3.3090822839851244, 'min_child_weight': 6, 'max_delta_step': 3}. Best is trial 2 with value: 0.9722621595814029.\n",
      "[I 2025-11-26 16:06:54,227] Trial 6 finished with value: 0.9734839263548772 and parameters: {'lambda': 4.3633233181080815, 'alpha': 0.07869784432435886, 'subsample': 0.7955533969174509, 'colsample_bytree': 0.8949625245780931, 'max_depth': 7, 'eta': 0.255008273958666, 'gamma': 1.5104710059072546, 'scale_pos_weight': 1.3673797129009313, 'min_child_weight': 5, 'max_delta_step': 9}. Best is trial 6 with value: 0.9734839263548772.\n",
      "[I 2025-11-26 16:06:54,796] Trial 7 finished with value: 0.9692743456153591 and parameters: {'lambda': 0.001137000314243987, 'alpha': 2.262271122868042, 'subsample': 0.598410867921694, 'colsample_bytree': 0.473783994874996, 'max_depth': 8, 'eta': 0.18199308331086342, 'gamma': 2.4552509944272582, 'scale_pos_weight': 2.952316898738102, 'min_child_weight': 10, 'max_delta_step': 6}. Best is trial 6 with value: 0.9734839263548772.\n",
      "[I 2025-11-26 16:06:55,221] Trial 8 finished with value: 0.9611012151104976 and parameters: {'lambda': 9.977411773082355, 'alpha': 8.044124455844594, 'subsample': 0.7248875938948129, 'colsample_bytree': 0.8911931106464263, 'max_depth': 9, 'eta': 0.17804252982706675, 'gamma': 7.044093408566054, 'scale_pos_weight': 3.3985176261097463, 'min_child_weight': 1, 'max_delta_step': 0}. Best is trial 6 with value: 0.9734839263548772.\n",
      "[I 2025-11-26 16:06:55,594] Trial 9 finished with value: 0.967855518012207 and parameters: {'lambda': 0.015379278017785656, 'alpha': 0.026015763548742958, 'subsample': 0.9393120207818951, 'colsample_bytree': 0.6854575855915119, 'max_depth': 7, 'eta': 0.2530152824950224, 'gamma': 8.316487763549482, 'scale_pos_weight': 2.0585523042585403, 'min_child_weight': 1, 'max_delta_step': 1}. Best is trial 6 with value: 0.9734839263548772.\n",
      "[I 2025-11-26 16:06:56,029] Trial 10 finished with value: 0.9533918620034445 and parameters: {'lambda': 0.4105947135368759, 'alpha': 0.0011544520841777288, 'subsample': 0.8389001641337814, 'colsample_bytree': 0.7998058526609341, 'max_depth': 5, 'eta': 0.11649648275158586, 'gamma': 2.854258294625861, 'scale_pos_weight': 8.485608764511944, 'min_child_weight': 4, 'max_delta_step': 10}. Best is trial 6 with value: 0.9734839263548772.\n",
      "[I 2025-11-26 16:06:56,761] Trial 11 finished with value: 0.9740956338860646 and parameters: {'lambda': 0.018193061045569944, 'alpha': 0.8425383544459737, 'subsample': 0.7999977964101324, 'colsample_bytree': 0.5602307205266787, 'max_depth': 10, 'eta': 0.2938417118985465, 'gamma': 0.06126886913643993, 'scale_pos_weight': 6.964231282677778, 'min_child_weight': 4, 'max_delta_step': 7}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:06:57,196] Trial 12 finished with value: 0.9672062250146912 and parameters: {'lambda': 0.021973450507933895, 'alpha': 0.4491567296803938, 'subsample': 0.8171000151255887, 'colsample_bytree': 0.9888773184486153, 'max_depth': 5, 'eta': 0.28318417403941465, 'gamma': 0.26992570597824384, 'scale_pos_weight': 7.0038015278899985, 'min_child_weight': 4, 'max_delta_step': 8}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:06:57,662] Trial 13 finished with value: 0.964342544352388 and parameters: {'lambda': 0.5516388272569942, 'alpha': 0.011664923694739836, 'subsample': 0.9860723843043483, 'colsample_bytree': 0.7630449723037717, 'max_depth': 6, 'eta': 0.20659494897388606, 'gamma': 3.5746383158611827, 'scale_pos_weight': 8.708021850000442, 'min_child_weight': 4, 'max_delta_step': 8}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:06:58,029] Trial 14 finished with value: 0.9532488203552832 and parameters: {'lambda': 0.1061666467640399, 'alpha': 0.6808244219859448, 'subsample': 0.8007133276095166, 'colsample_bytree': 0.5680496362184535, 'max_depth': 3, 'eta': 0.2978519568398129, 'gamma': 1.1905172005734874, 'scale_pos_weight': 6.055385862227684, 'min_child_weight': 3, 'max_delta_step': 7}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:06:58,571] Trial 15 finished with value: 0.9716995156536102 and parameters: {'lambda': 0.006393059730479925, 'alpha': 1.326391962822067, 'subsample': 0.7700236890890856, 'colsample_bytree': 0.7819589109296272, 'max_depth': 10, 'eta': 0.14725419660370923, 'gamma': 3.89258512188473, 'scale_pos_weight': 1.210793480944265, 'min_child_weight': 6, 'max_delta_step': 9}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:06:59,021] Trial 16 finished with value: 0.9677472301648403 and parameters: {'lambda': 0.04828427434111057, 'alpha': 0.01039178734258075, 'subsample': 0.9119924088654938, 'colsample_bytree': 0.5576153241678652, 'max_depth': 6, 'eta': 0.22801626410643958, 'gamma': 1.126809703636775, 'scale_pos_weight': 7.201533682806239, 'min_child_weight': 3, 'max_delta_step': 3}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:06:59,598] Trial 17 finished with value: 0.955230718039639 and parameters: {'lambda': 0.5024550537299917, 'alpha': 0.2807801811947171, 'subsample': 0.6564933988188623, 'colsample_bytree': 0.8531736645839404, 'max_depth': 7, 'eta': 0.060668918080870965, 'gamma': 1.0799238399815958, 'scale_pos_weight': 9.720864726529115, 'min_child_weight': 5, 'max_delta_step': 7}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:00,351] Trial 18 finished with value: 0.9674982264408173 and parameters: {'lambda': 0.005318248999024318, 'alpha': 7.910572535151313, 'subsample': 0.46702392531761183, 'colsample_bytree': 0.9941516985012969, 'max_depth': 8, 'eta': 0.27208897065092225, 'gamma': 3.8702744739208805, 'scale_pos_weight': 4.440345299995815, 'min_child_weight': 7, 'max_delta_step': 9}. Best is trial 11 with value: 0.9740956338860646.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 16:07:00,885] Trial 19 finished with value: 0.9546145896845294 and parameters: {'lambda': 1.5406907512842916, 'alpha': 0.08853371390228847, 'subsample': 0.8737348252959423, 'colsample_bytree': 0.7439328762724617, 'max_depth': 4, 'eta': 0.19249974450379387, 'gamma': 1.888812373457732, 'scale_pos_weight': 6.7676078051476285, 'min_child_weight': 3, 'max_delta_step': 6}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:02,187] Trial 20 finished with value: 0.9730676849400551 and parameters: {'lambda': 0.048104128135911785, 'alpha': 1.0076195084381088, 'subsample': 0.7729262257974018, 'colsample_bytree': 0.6228150934207498, 'max_depth': 10, 'eta': 0.15521312139911886, 'gamma': 0.11650025827771593, 'scale_pos_weight': 4.785053591909616, 'min_child_weight': 5, 'max_delta_step': 3}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:03,387] Trial 21 finished with value: 0.9703261384087579 and parameters: {'lambda': 0.049538440562402655, 'alpha': 0.9617006021280275, 'subsample': 0.7910136232041909, 'colsample_bytree': 0.6129829568606884, 'max_depth': 10, 'eta': 0.11403056999122599, 'gamma': 0.10527151522102872, 'scale_pos_weight': 4.9827400612808255, 'min_child_weight': 5, 'max_delta_step': 3}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:04,113] Trial 22 finished with value: 0.9655445755596714 and parameters: {'lambda': 0.1933562599833276, 'alpha': 3.0685351305947974, 'subsample': 0.748651017211933, 'colsample_bytree': 0.5256754311903473, 'max_depth': 9, 'eta': 0.15048935937799882, 'gamma': 1.2449520776448149, 'scale_pos_weight': 8.050889369303722, 'min_child_weight': 5, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:04,821] Trial 23 finished with value: 0.9728369394733266 and parameters: {'lambda': 0.037448539903060005, 'alpha': 0.28200080085961454, 'subsample': 0.8480068770097301, 'colsample_bytree': 0.7137534759331695, 'max_depth': 10, 'eta': 0.07649339642498018, 'gamma': 2.944118666993578, 'scale_pos_weight': 1.4916553510188546, 'min_child_weight': 2, 'max_delta_step': 4}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:05,356] Trial 24 finished with value: 0.9698600333260154 and parameters: {'lambda': 0.2335112880600995, 'alpha': 0.03051936031391931, 'subsample': 0.678216383440314, 'colsample_bytree': 0.40516682175531527, 'max_depth': 8, 'eta': 0.23580410642648564, 'gamma': 0.6980228958917554, 'scale_pos_weight': 6.585180294678449, 'min_child_weight': 7, 'max_delta_step': 7}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:05,805] Trial 25 finished with value: 0.9704172055497838 and parameters: {'lambda': 0.005314829835808644, 'alpha': 3.587712312651992, 'subsample': 0.9685357262615254, 'colsample_bytree': 0.6403357190579065, 'max_depth': 7, 'eta': 0.29943834143453674, 'gamma': 1.8301136219288723, 'scale_pos_weight': 4.949854203810807, 'min_child_weight': 6, 'max_delta_step': 9}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:06,553] Trial 26 finished with value: 0.7863888472836854 and parameters: {'lambda': 1.1850328346576748, 'alpha': 0.47305721601640316, 'subsample': 0.7615625772063886, 'colsample_bytree': 0.5217675476643425, 'max_depth': 9, 'eta': 0.013977188269528346, 'gamma': 0.7452141171426039, 'scale_pos_weight': 7.517145651296388, 'min_child_weight': 4, 'max_delta_step': 6}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:07,340] Trial 27 finished with value: 0.9735674055989065 and parameters: {'lambda': 0.05771615211845021, 'alpha': 0.1637845268053855, 'subsample': 0.5058725385068327, 'colsample_bytree': 0.8200240682975032, 'max_depth': 10, 'eta': 0.27217080983557146, 'gamma': 0.018520868156671713, 'scale_pos_weight': 9.959950519915589, 'min_child_weight': 5, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:07,833] Trial 28 finished with value: 0.96683618062692 and parameters: {'lambda': 7.42489093194252, 'alpha': 0.17285839435954484, 'subsample': 0.538682948196009, 'colsample_bytree': 0.8649050330832796, 'max_depth': 6, 'eta': 0.26743375222770693, 'gamma': 1.791559875306277, 'scale_pos_weight': 9.331731777421597, 'min_child_weight': 2, 'max_delta_step': 0}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:08,215] Trial 29 finished with value: 0.947058066475372 and parameters: {'lambda': 3.2259144349552678, 'alpha': 0.09099907233961932, 'subsample': 0.5335585442657536, 'colsample_bytree': 0.8180329874537824, 'max_depth': 3, 'eta': 0.26204063397770333, 'gamma': 5.875543149746043, 'scale_pos_weight': 9.9500759057943, 'min_child_weight': 8, 'max_delta_step': 1}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:08,627] Trial 30 finished with value: 0.9565853623732515 and parameters: {'lambda': 0.023676967421144078, 'alpha': 0.04500811260382946, 'subsample': 0.402410916848782, 'colsample_bytree': 0.9387678442449188, 'max_depth': 4, 'eta': 0.21535770054248893, 'gamma': 4.784151436840726, 'scale_pos_weight': 9.020612868762067, 'min_child_weight': 3, 'max_delta_step': 4}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:09,330] Trial 31 finished with value: 0.9739478093881292 and parameters: {'lambda': 0.07654552575166673, 'alpha': 1.4038014180902343, 'subsample': 0.8771357263813813, 'colsample_bytree': 0.6759393574501621, 'max_depth': 10, 'eta': 0.27470029322470013, 'gamma': 0.03319867246915442, 'scale_pos_weight': 7.722458106343824, 'min_child_weight': 5, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:10,096] Trial 32 finished with value: 0.9739928784140627 and parameters: {'lambda': 0.08466798341665008, 'alpha': 0.4411582224129362, 'subsample': 0.8773915289444726, 'colsample_bytree': 0.7179550960968162, 'max_depth': 10, 'eta': 0.2767963060843375, 'gamma': 0.6281048130109392, 'scale_pos_weight': 6.144411858081755, 'min_child_weight': 6, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:11,611] Trial 33 finished with value: 0.9738593575771824 and parameters: {'lambda': 0.07489783183035537, 'alpha': 0.5482503340124648, 'subsample': 0.8957760097305288, 'colsample_bytree': 0.7133636575643221, 'max_depth': 10, 'eta': 0.2415557177987071, 'gamma': 0.684255997012216, 'scale_pos_weight': 7.913918859302543, 'min_child_weight': 6, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:12,303] Trial 34 finished with value: 0.9736395012542965 and parameters: {'lambda': 0.08541487562956136, 'alpha': 1.461392448232066, 'subsample': 0.8925704736117597, 'colsample_bytree': 0.7144908869885882, 'max_depth': 10, 'eta': 0.2403891533041873, 'gamma': 0.7548111691530832, 'scale_pos_weight': 6.075241576489237, 'min_child_weight': 8, 'max_delta_step': 1}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:12,802] Trial 35 finished with value: 0.971104139602605 and parameters: {'lambda': 0.009127984833585992, 'alpha': 0.6698694676673018, 'subsample': 0.9115537127911866, 'colsample_bytree': 0.6740860434374116, 'max_depth': 9, 'eta': 0.281896808243585, 'gamma': 2.371687192770069, 'scale_pos_weight': 7.814254536091138, 'min_child_weight': 6, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:13,465] Trial 36 finished with value: 0.9731117156687789 and parameters: {'lambda': 0.18530745968042847, 'alpha': 3.871744215071521, 'subsample': 0.8605547846544181, 'colsample_bytree': 0.7365740565148506, 'max_depth': 10, 'eta': 0.24333629194839929, 'gamma': 0.6108622549109406, 'scale_pos_weight': 5.669536880026501, 'min_child_weight': 7, 'max_delta_step': 4}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:13,923] Trial 37 finished with value: 0.970168177491562 and parameters: {'lambda': 0.002465809112157198, 'alpha': 0.4412437853764315, 'subsample': 0.9645200794220299, 'colsample_bytree': 0.5941871553292556, 'max_depth': 9, 'eta': 0.2794703041193732, 'gamma': 2.6517063634273383, 'scale_pos_weight': 6.384509477205653, 'min_child_weight': 6, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 16:07:14,458] Trial 38 finished with value: 0.9710017936145693 and parameters: {'lambda': 0.027201757414995906, 'alpha': 1.8551628514159455, 'subsample': 0.9247312551837084, 'colsample_bytree': 0.6543060127437591, 'max_depth': 8, 'eta': 0.25216765301954475, 'gamma': 0.561492759762825, 'scale_pos_weight': 7.476449240441482, 'min_child_weight': 9, 'max_delta_step': 5}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:15,143] Trial 39 finished with value: 0.970765005968051 and parameters: {'lambda': 0.09331776845523787, 'alpha': 0.7445977030801677, 'subsample': 0.9511090318859826, 'colsample_bytree': 0.684568977534018, 'max_depth': 10, 'eta': 0.20027012833691818, 'gamma': 2.156350912664511, 'scale_pos_weight': 8.291519082248723, 'min_child_weight': 7, 'max_delta_step': 1}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:15,801] Trial 40 finished with value: 0.9692278963733996 and parameters: {'lambda': 0.012180320306941383, 'alpha': 0.2652740384612712, 'subsample': 0.8269428924728164, 'colsample_bytree': 0.539562895034262, 'max_depth': 9, 'eta': 0.2229938324400048, 'gamma': 3.3354257925107786, 'scale_pos_weight': 7.817200653309198, 'min_child_weight': 4, 'max_delta_step': 0}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:16,386] Trial 41 finished with value: 0.9734337937704948 and parameters: {'lambda': 0.07910862367776139, 'alpha': 1.5215997086492479, 'subsample': 0.8979199659669942, 'colsample_bytree': 0.7028716474918981, 'max_depth': 10, 'eta': 0.24494229652580554, 'gamma': 1.441411840334581, 'scale_pos_weight': 6.01444877551355, 'min_child_weight': 8, 'max_delta_step': 1}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:17,015] Trial 42 finished with value: 0.973243604332902 and parameters: {'lambda': 0.2579957765926127, 'alpha': 5.276043536415498, 'subsample': 0.8803900801499838, 'colsample_bytree': 0.7331993792230161, 'max_depth': 10, 'eta': 0.2957992354990585, 'gamma': 0.7665730425790043, 'scale_pos_weight': 5.331486234500063, 'min_child_weight': 9, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:17,585] Trial 43 finished with value: 0.9717789192616069 and parameters: {'lambda': 0.16020437901815499, 'alpha': 2.491099008731365, 'subsample': 0.9994301513201915, 'colsample_bytree': 0.4806602862479617, 'max_depth': 9, 'eta': 0.26166697192200844, 'gamma': 0.4842043867255693, 'scale_pos_weight': 6.2245288706792685, 'min_child_weight': 8, 'max_delta_step': 1}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:18,140] Trial 44 finished with value: 0.9715883420560495 and parameters: {'lambda': 0.0726648387257621, 'alpha': 1.1317904896232986, 'subsample': 0.8845061517971674, 'colsample_bytree': 0.6635035563249118, 'max_depth': 10, 'eta': 0.23079991264113298, 'gamma': 1.4704908079119343, 'scale_pos_weight': 7.290077885845804, 'min_child_weight': 7, 'max_delta_step': 0}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:18,585] Trial 45 finished with value: 0.9669246577886552 and parameters: {'lambda': 0.32217306693618825, 'alpha': 0.39768596061046346, 'subsample': 0.8419933246720527, 'colsample_bytree': 0.7743678235288765, 'max_depth': 9, 'eta': 0.2842083669269368, 'gamma': 6.0944701645855135, 'scale_pos_weight': 6.684249639967218, 'min_child_weight': 10, 'max_delta_step': 3}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:18,989] Trial 46 finished with value: 0.9640790017133497 and parameters: {'lambda': 0.1382145740063992, 'alpha': 1.5008731892279734, 'subsample': 0.9347363455118575, 'colsample_bytree': 0.7133695633863287, 'max_depth': 10, 'eta': 0.25019753143825174, 'gamma': 9.785286453140612, 'scale_pos_weight': 5.445220013987823, 'min_child_weight': 6, 'max_delta_step': 1}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:19,595] Trial 47 finished with value: 0.9685507654388773 and parameters: {'lambda': 0.028267750719965234, 'alpha': 5.199626378109571, 'subsample': 0.8144855985151374, 'colsample_bytree': 0.587924101256389, 'max_depth': 9, 'eta': 0.1781173411548257, 'gamma': 1.007877443794301, 'scale_pos_weight': 7.02593393078666, 'min_child_weight': 4, 'max_delta_step': 5}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:20,043] Trial 48 finished with value: 0.9649872902795297 and parameters: {'lambda': 0.7634182045004239, 'alpha': 0.6467259233152824, 'subsample': 0.7318237861223083, 'colsample_bytree': 0.7539625331104499, 'max_depth': 10, 'eta': 0.23847959113491957, 'gamma': 6.592421618649773, 'scale_pos_weight': 8.688103995044495, 'min_child_weight': 8, 'max_delta_step': 2}. Best is trial 11 with value: 0.9740956338860646.\n",
      "[I 2025-11-26 16:07:20,508] Trial 49 finished with value: 0.9687162885580253 and parameters: {'lambda': 0.017162437614099663, 'alpha': 2.125903771175778, 'subsample': 0.8670533690444913, 'colsample_bytree': 0.6953989586505895, 'max_depth': 8, 'eta': 0.2164737881717938, 'gamma': 4.566292138410576, 'scale_pos_weight': 4.016497779710399, 'min_child_weight': 5, 'max_delta_step': 4}. Best is trial 11 with value: 0.9740956338860646.\n"
     ]
    }
   ],
   "source": [
    "# --- Attempt 4: Optuna Hyperparameter Tuning on XGBoost (with SMOTE-Tomek data) ---\n",
    "\n",
    "# Define Optuna objective for XGBoost\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'verbosity': 0,\n",
    "        'booster': 'gbtree',\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    \n",
    "    # Calculate the cross-validated f1_score\n",
    "    f1_scorer = make_scorer(f1_score, average='macro')\n",
    "    scores = cross_val_score(model, X_train_smt, y_train_smt, cv=3, scoring=f1_scorer, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna study\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83ac7a71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  F1-score: 0.9740956338860646\n",
      "  Params: \n",
      "    lambda: 0.018193061045569944\n",
      "    alpha: 0.8425383544459737\n",
      "    subsample: 0.7999977964101324\n",
      "    colsample_bytree: 0.5602307205266787\n",
      "    max_depth: 10\n",
      "    eta: 0.2938417118985465\n",
      "    gamma: 0.06126886913643993\n",
      "    scale_pos_weight: 6.964231282677778\n",
      "    min_child_weight: 4\n",
      "    max_delta_step: 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97     11423\n",
      "           1       0.68      0.87      0.76      1074\n",
      "\n",
      "    accuracy                           0.95     12497\n",
      "   macro avg       0.83      0.92      0.87     12497\n",
      "weighted avg       0.96      0.95      0.96     12497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best trial:')\n",
    "trial = study_xgb.best_trial\n",
    "print('  F1-score: {}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "    \n",
    "\n",
    "# Train best model on full SMOTE-Tomek data\n",
    "best_params = study_xgb.best_params\n",
    "best_model_xgb = XGBClassifier(**best_params)\n",
    "best_model_xgb.fit(X_train_smt, y_train_smt)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model_xgb.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6376e1b",
   "metadata": {},
   "source": [
    "We also performed a full Optuna search on XGBoost (50 trials, 11 hyperparameters).\n",
    "\n",
    "The final model achieved:\n",
    "\n",
    "* **Recall (Default = 1): 87%**\n",
    "* **Precision (Default = 1): 68%**\n",
    "* **F1 Score (Default = 1): 76%**\n",
    "\n",
    "While XGBoost delivered a stronger **precision**, it still **did not meet the required â‰¥ 90% recall**.\n",
    "\n",
    "Additionally:\n",
    "\n",
    "* XGBoost is less explainable\n",
    "* Converting it into BRE-friendly rules is harder\n",
    "* It increases deployment complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed27fe",
   "metadata": {},
   "source": [
    "###  **Final Model Selection**\n",
    "\n",
    "Although XGBoost achieved slightly higher precision, **Logistic Regression outperformed it on recall**, which is the **most critical metric for the business**.\n",
    "\n",
    "Logistic Regression also:\n",
    "\n",
    "* Provides direct interpretability\n",
    "* Allows extracting coefficients for rule-based systems\n",
    "* Is easy to monitor, debug, and explain\n",
    "* Fits perfectly with the need for â€œAI explainabilityâ€\n",
    "\n",
    "### **Final Selected Model: Logistic Regression (SMOTE-Tomek + Optuna-tuned)**\n",
    "\n",
    "This model fully satisfies all business KPIs and operational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63ad050a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Logistic Regression model saved.\n",
      "[CV] END ...........C=0.012742749857031334, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.004832930238571752, solver=liblinear; total time=   0.3s\n",
      "[CV] END ..........C=0.0006951927961775605, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=0.00026366508987303583, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ..................C=11.288378916846883, solver=saga; total time=   0.2s\n",
      "[CV] END ...................C=1438.44988828766, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ...................C=4.281332398719396, solver=saga; total time=   0.2s\n",
      "[CV] END .............C=29.763514416313132, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ..................C=545.5594781168514, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............C=0.23357214690901212, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..............C=1.623776739188721, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...............C=0.004832930238571752, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..................C=78.47599703514607, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .........C=0.00026366508987303583, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .............C=11.288378916846883, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..............C=545.5594781168514, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...............C=0.0018329807108324356, solver=saga; total time=   0.2s\n",
      "[CV] END .........................C=0.0001, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ..............C=0.0006951927961775605, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...................C=545.5594781168514, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=78.47599703514607, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............................C=0.0001, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............C=0.08858667904100823, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ..........C=0.0006951927961775605, solver=liblinear; total time=   0.1s\n",
      "[CV] END .............C=0.00026366508987303583, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............C=0.0018329807108324356, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .................C=0.23357214690901212, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.23357214690901212, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ...................C=3792.690190732246, solver=saga; total time=   0.2s\n",
      "[CV] END ..................C=1.623776739188721, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ..................C=545.5594781168514, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............C=0.23357214690901212, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...............C=206.913808111479, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...............C=0.004832930238571752, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............C=0.615848211066026, solver=newton-cg; total time=   0.4s\n",
      "[CV] END .........C=0.00026366508987303583, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ...........C=0.004832930238571752, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ..............C=545.5594781168514, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...............C=0.0018329807108324356, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=0.615848211066026, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=0.615848211066026, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..................C=3792.690190732246, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............C=0.08858667904100823, solver=liblinear; total time=   0.2s\n",
      "[CV] END ................C=0.08858667904100823, solver=lbfgs; total time=   0.4s\n",
      "[CV] END ...............C=1438.44988828766, solver=liblinear; total time=   0.3s\n",
      "[CV] END ..............C=545.5594781168514, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ...................C=4.281332398719396, solver=saga; total time=   0.2s\n",
      "[CV] END .............C=29.763514416313132, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ..................C=545.5594781168514, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............C=4.281332398719396, solver=newton-cg; total time=   0.4s\n",
      "[CV] END .............C=29.763514416313132, solver=liblinear; total time=   0.2s\n",
      "[CV] END ........................C=10000.0, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ..................C=29.763514416313132, solver=saga; total time=   0.3s\n",
      "[CV] END ..................C=4.281332398719396, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................C=10000.0, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ................C=0.004832930238571752, solver=saga; total time=   0.2s\n",
      "[CV] END .........C=0.00026366508987303583, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..................C=3792.690190732246, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .............................C=0.0001, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ...........C=0.012742749857031334, solver=liblinear; total time=   0.2s\n",
      "[CV] END ................C=0.08858667904100823, solver=lbfgs; total time=   0.4s\n",
      "[CV] END ...............C=1438.44988828766, solver=liblinear; total time=   0.3s\n",
      "[CV] END .................C=0.23357214690901212, solver=saga; total time=   0.2s\n",
      "[CV] END ...................C=1438.44988828766, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ...................C=4.281332398719396, solver=saga; total time=   0.2s\n",
      "[CV] END ....................C=206.913808111479, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.0006951927961775605, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=4.281332398719396, solver=newton-cg; total time=   0.4s\n",
      "[CV] END .............C=29.763514416313132, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=78.47599703514607, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .........C=0.00026366508987303583, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .............C=11.288378916846883, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..............C=545.5594781168514, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..................C=4.281332398719396, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .........................C=0.0001, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ..............C=0.615848211066026, solver=liblinear; total time=   0.2s\n",
      "[CV] END .........C=0.00026366508987303583, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................C=3792.690190732246, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............C=0.08858667904100823, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.004832930238571752, solver=liblinear; total time=   0.3s\n",
      "[CV] END ..........C=0.0006951927961775605, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..................C=11.288378916846883, solver=saga; total time=   0.3s\n",
      "[CV] END ..............C=545.5594781168514, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ....................C=206.913808111479, solver=saga; total time=   0.2s\n",
      "[CV] END ..................C=1.623776739188721, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ..............C=0.00026366508987303583, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.23357214690901212, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..............C=1.623776739188721, solver=liblinear; total time=   0.3s\n",
      "[CV] END ..............C=0.615848211066026, solver=newton-cg; total time=   0.4s\n",
      "[CV] END .............C=11.288378916846883, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.004832930238571752, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .................C=0.03359818286283781, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.0018329807108324356, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=0.0006951927961775605, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............C=0.0006951927961775605, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .........C=0.00026366508987303583, solver=liblinear; total time=   0.1s\n",
      "[CV] END ..................C=0.615848211066026, solver=lbfgs; total time=   0.1s\n",
      "[CV] END .............................C=0.0001, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............C=0.08858667904100823, solver=newton-cg; total time=   0.5s\n",
      "[CV] END ...............C=1438.44988828766, solver=liblinear; total time=   0.3s\n",
      "[CV] END .................C=0.23357214690901212, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.23357214690901212, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ...................C=3792.690190732246, solver=saga; total time=   0.2s\n",
      "[CV] END .............C=29.763514416313132, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ..............C=0.00026366508987303583, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=206.913808111479, solver=liblinear; total time=   0.2s\n",
      "[CV] END .............C=29.763514416313132, solver=liblinear; total time=   0.2s\n",
      "[CV] END ........................C=10000.0, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ..................C=29.763514416313132, solver=saga; total time=   0.2s\n",
      "[CV] END .................C=0.03359818286283781, solver=saga; total time=   0.2s\n",
      "[CV] END ............................C=10000.0, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............C=0.615848211066026, solver=liblinear; total time=   0.1s\n",
      "[CV] END ...................C=545.5594781168514, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=78.47599703514607, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.08858667904100823, solver=liblinear; total time=   0.2s\n",
      "[CV] END ...........C=0.004832930238571752, solver=liblinear; total time=   0.1s\n",
      "[CV] END ................C=0.08858667904100823, solver=lbfgs; total time=   0.4s\n",
      "[CV] END ..............C=0.0018329807108324356, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ..............C=545.5594781168514, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ...................C=1438.44988828766, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=206.913808111479, solver=saga; total time=   0.2s\n",
      "[CV] END ...............C=0.0006951927961775605, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=0.00026366508987303583, solver=saga; total time=   0.3s\n",
      "[CV] END ...............C=206.913808111479, solver=liblinear; total time=   0.3s\n",
      "[CV] END ...............C=0.004832930238571752, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..................C=78.47599703514607, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ........................C=10000.0, solver=newton-cg; total time=   0.4s\n",
      "[CV] END ...........C=0.004832930238571752, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ..................C=4.281332398719396, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ............................C=10000.0, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ................C=0.004832930238571752, solver=saga; total time=   0.2s\n",
      "[CV] END ..................C=0.615848211066026, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..............C=78.47599703514607, solver=liblinear; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...........C=0.012742749857031334, solver=liblinear; total time=   0.1s\n",
      "[CV] END ............C=0.08858667904100823, solver=newton-cg; total time=   0.5s\n",
      "[CV] END .............C=0.00026366508987303583, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............C=0.0018329807108324356, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ..................C=11.288378916846883, solver=saga; total time=   0.2s\n",
      "[CV] END ............C=0.23357214690901212, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ...................C=3792.690190732246, solver=saga; total time=   0.2s\n",
      "[CV] END ..................C=1.623776739188721, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ...............C=0.0006951927961775605, solver=saga; total time=   0.2s\n",
      "[CV] END ..............C=4.281332398719396, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ..............C=1.623776739188721, solver=liblinear; total time=   0.2s\n",
      "[CV] END ..............C=0.615848211066026, solver=newton-cg; total time=   0.5s\n",
      "[CV] END ..................C=29.763514416313132, solver=saga; total time=   0.3s\n",
      "[CV] END .................C=0.03359818286283781, solver=saga; total time=   0.2s\n",
      "[CV] END .........................C=0.0001, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ................C=0.004832930238571752, solver=saga; total time=   0.2s\n",
      "[CV] END ...................C=545.5594781168514, solver=saga; total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "# Save the tuned logistic model for use in the next notebook\n",
    "dump(best_model_logistic, \"../outputs/models/best_model_logistic.pkl\")\n",
    "print(\"Final Logistic Regression model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34c41ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and test sets for model evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Save the train, test split for model evaluation\n",
    "X_train_smt.to_parquet(\"../data/processed/final_X_smt_train.parquet\", index=False)\n",
    "y_train_smt.to_frame().to_parquet(\"../data/processed/final_y_smt_train.parquet\", index=False)\n",
    "\n",
    "X_test.to_parquet(\"../data/processed/final_model_X_test.parquet\", index=False)\n",
    "y_test.to_frame().to_parquet(\"../data/processed/final_model_y_test.parquet\", index=False)\n",
    "\n",
    "print(\"Saved model and test sets for model evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20b843a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../outputs/models/best_params_logistic.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Final Optuna Best Parameters\n",
    "dump(study_logistic.best_params, \"../outputs/models/best_params_logistic.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
